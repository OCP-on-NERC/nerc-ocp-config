apiVersion: v1
kind: ConfigMap
metadata:
  name: thanos-ruler-custom-rules
  namespace: open-cluster-management-observability
data:
  custom_rules.yaml: |
    groups:
      - name: openshift-storage
        rules:

        - alert: CustomStoragePersistentVolumeFillingUp
          annotations:
            summary: PersistentVolume is filling up.
            description: "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"db-noobaa-db-pg-.*|metrics-backing-store-noobaa-pvc-.*|noobaa-default-backing-store-noobaa-pvc-.*"}/kubelet_volume_stats_capacity_bytes < 0.10
          for: 1m
          labels:
            instance: "{{ $labels.instance }}"
            cluster: "{{ $labels.cluster }}"
            clusterID: "{{ $labels.clusterID }}"
            PersistentVolumeClaim: "{{ $labels.persistentvolumeclaim }}"
            severity: critical

        - alert: CustomStoragePersistentVolumeFillingUpPredicted
          annotations:
            summary: PersistentVolume is filling up and is predicted to run out of space in 6h.
            description: "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
          expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"db-noobaa-db-pg-.*|metrics-backing-store-noobaa-pvc-.*|noobaa-default-backing-store-noobaa-pvc-.*"}/kubelet_volume_stats_capacity_bytes) < 0.10 and (predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600)) < 0
          for: 1h
          labels:
            instance: "{{ $labels.instance }}"
            cluster: "{{ $labels.cluster }}"
            clusterID: "{{ $labels.clusterID }}"
            PersistentVolumeClaim: "{{ $labels.persistentvolumeclaim }}"
            severity: warning

        - alert: CustomContainerMemoryUsage
          annotations:
            summary: Container Memory usage (instance {{ $labels.instance }})
            description: "Container Memory usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          expr: (sum(container_memory_working_set_bytes{name!=""}) BY (cluster, pod, container) / sum(kube_pod_container_resource_limits{resource="memory"} > 0) BY (cluster, pod, container) * 100) > 80
          for: 2m
          labels:
            instance: "{{ $labels.instance }}"
            cluster: "{{ $labels.cluster }}"
            clusterID: "{{ $labels.clusterID }}"
            severity: warning

        - alert: CustomContainerCpuUsage
          annotations:
            summary: Container CPU usage (instance {{ $labels.container_name }})
            description: "Container CPU usage is above 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          expr: sum(avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate[1h])) by (cluster, namespace, pod) / avg(kube_pod_container_resource_limits{resource="cpu"}) by (cluster, namespace, pod) * 100 > 80
          for: 5m
          labels:
            instance: "{{ $labels.instance }}"
            cluster: "{{ $labels.cluster }}"
            clusterID: "{{ $labels.clusterID }}"
            severity: warning

        - alert: CustomCephStorageFillingUp
          annotations:
            summary: Ceph Storage is filling up.
            description: "The {{ $labels.pool }} pool Ceph Storage is {{ $value | humanizePercentage }} full."
          expr: (ceph_pool_raw_used_bytes{pool=~"(nerc_ocp_prod_.*)|(nerc_ocp_infra_.*)"}) / on (cluster, namespace, pool) (ceph_pool_quota_max_bytes{pool=~"(nerc_ocp_prod_.*)|(nerc_ocp_infra_.*)"}) > 0.80
          for: 1m
          labels:
            severity: warning

        - alert: CustomNetworkInterfaceErrors
          annotations:
            summary: Node network transmit errors
            description: "{{ $value }} node network transmit errors occured"
          expr: increase(node_network_transmit_errs_total[1h]) > 0
          for: 1m
          labels:
            instance: "{{ $labels.instance }}"
            cluster: "{{ $labels.cluster }}"
            clusterID: "{{ $labels.clusterID }}"
            severity: warning

# Prometheus generates a metric called up that indicates whether a scrape was successful.
# A value of “1” is scrape indicates success, “0” failure.
# The up metric is useful for debugging and alerting for targets that are down or having issues.
# Each target should produce an up metric on every scrape.
# I'm not sure which of the many metrics we want to check are up, but there are many here:
# https://multicloud-console.apps.nerc-ocp-infra.rc.fas.harvard.edu/grafana/explore?orgId=1&left=%5B%22now-1h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22up%7Bpod!%3D%5C%22%5C%22%7D%20%3D%3D%200%22%7D%5D
#        - alert: CustomInstanceDown
#          annotations:
#            summary: "Instance [{{ $labels.instance }}] down"
#            description: "[{{ $labels.instance }}] of job [{{ $labels.job }}] has been down for more than 15 minute."
#          expr: up == 0
#          for: 15m
#          labels:
#            severity: critical
