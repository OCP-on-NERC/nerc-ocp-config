apiVersion: v1
kind: ConfigMap
metadata:
  name: thanos-ruler-custom-rules
  namespace: open-cluster-management-observability
data:
  custom_rules.yaml: |
    groups:
      - name: openshift-storage
        rules:

        - alert: CustomStoragePersistentVolumeFillingUp
          annotations:
            summary: "{{ $labels.cluster }} PersistentVolume is filling up"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22kubelet_volume_stats_available_bytes%7Bpersistentvolumeclaim!~%5C%22wal-logging-loki-ingester-.*%5C%22%7D%2Fkubelet_volume_stats_capacity_bytes%20%3C%200.10%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
          expr: kubelet_volume_stats_available_bytes{persistentvolumeclaim!~"wal-logging-loki-ingester-.*"}/kubelet_volume_stats_capacity_bytes < 0.10
          for: 1m
          labels:
            severity: critical

        - alert: CustomStoragePersistentVolumeFillingUpPredicted
          annotations:
            summary: "{{ $labels.cluster }} PersistentVolume is filling up and is predicted to run out of space in 6h"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22(kubelet_volume_stats_available_bytes%7Bpersistentvolumeclaim!~%5C%22wal-logging-loki-ingester-.*%5C%22%7D%2Fkubelet_volume_stats_capacity_bytes)%20%3C%200.10%20and%20(predict_linear(kubelet_volume_stats_available_bytes%5B6h%5D,%204%20*%2024%20*%203600))%20%3C%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
          expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim!~"wal-logging-loki-ingester-.*"}/kubelet_volume_stats_capacity_bytes) < 0.10 and (predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600)) < 0
          for: 1h
          labels:
            severity: warning

      - name: memory
        rules:

        - alert: CustomContainerMemoryUsage
          annotations:
            summary: "{{ $labels.cluster }} container Memory usage above 80%"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22(sum(container_memory_working_set_bytes%7Bname!%3D%5C%22%5C%22%7D)%20BY%20(cluster,%20namespace,%20pod,%20container)%20%2F%20sum(kube_pod_container_resource_limits%7Bresource%3D%5C%22memory%5C%22%7D%20%3E%200)%20BY%20(cluster,%20namespace,%20pod,%20container))%20%3E%200.80%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              Container Memory usage is at {{ $value | humanizePercentage }} in namespace {{ $labels.namespace }}, pod {{ $labels.pod }}, container {{ $labels.container }}"
          expr: (sum(container_memory_working_set_bytes{name!=""}) BY (cluster, namespace, pod, container) / sum(kube_pod_container_resource_limits{resource="memory"} > 0) BY (cluster, namespace, pod, container)) > 0.80
          for: 2m
          labels:
            severity: warning
            groupname: memory

      - name: cpu
        rules:

        - alert: CustomContainerCpuUsage
          annotations:
            summary: "{{ $labels.cluster }} container CPU usage above 80%"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22sum(avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate%5B1h%5D))%20by%20(cluster,%20namespace,%20pod)%20%2F%20avg(kube_pod_container_resource_limits%7Bresource%3D%5C%22cpu%5C%22%7D)%20by%20(cluster,%20namespace,%20pod)%20%3E%200.80%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              Container CPU usage is at {{ $value | humanizePercentage }} in namespace {{ $labels.namespace }}, pod {{ $labels.pod }}.
          expr: sum(avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate[1h])) by (cluster, namespace, pod) / avg(kube_pod_container_resource_limits{resource="cpu"}) by (cluster, namespace, pod) > 0.80
          for: 5m
          labels:
            severity: warning
            cpu: memory

      - name: ceph-storage
        rules:

        - alert: CustomCephStorageFillingUp
          annotations:
            summary: "{{ $labels.cluster }} Ceph Storage is filling up"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22(ceph_pool_raw_used_bytes%7Bpool%3D~%5C%22(nerc_ocp_prod_.*)%7C(nerc_ocp_infra_.*)%5C%22%7D)%20%2F%20on%20(cluster,%20namespace,%20pool)%20(ceph_pool_quota_max_bytes%7Bpool%3D~%5C%22(nerc_ocp_prod_.*)%7C(nerc_ocp_infra_.*)%5C%22%7D)%20%3E%200.80%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              The {{ $labels.cluster }} {{ $labels.pool }} pool Ceph Storage is {{ $value | humanizePercentage }} full.
          expr: (ceph_pool_raw_used_bytes{pool=~"(nerc_ocp_prod_.*)|(nerc_ocp_infra_.*)"}) / on (cluster, namespace, pool) (ceph_pool_quota_max_bytes{pool=~"(nerc_ocp_prod_.*)|(nerc_ocp_infra_.*)"}) > 0.80
          for: 1m
          labels:
            severity: warning

        - alert: CustomCephStorageFillingUpPredicted
          annotations:
            summary: "{{ $labels.cluster }} Ceph Storage is filling up and is predicted to run out of space in 90 days"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-1h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22ceph_pool_quota_max_bytes%7Bpool%3D~%5C%22(nerc_ocp_prod_.*)%7C(nerc_ocp_infra_.*)%5C%22%7D%20-%20on%20(cluster,%20namespace,%20pool,%20pod)%20predict_linear(ceph_pool_used_bytes%7Bpool%3D~%5C%22(nerc_ocp_prod_.*)%7C(nerc_ocp_infra_.*)%5C%22%7D%5B90d%5D,%2090%20*%2024%20*%2060%20*%2060)%20%3C%3D%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              The {{ $labels.cluster }} {{ $labels.pool }} pool Ceph Storage is predicted to run out of space in 90 days
          expr: ceph_pool_quota_max_bytes{pool=~"(nerc_ocp_prod_.*)|(nerc_ocp_infra_.*)"} - on (cluster, namespace, pool, pod) predict_linear(ceph_pool_used_bytes{pool=~"(nerc_ocp_prod_.*)|(nerc_ocp_infra_.*)"}[90d], 90 * 24 * 60 * 60) <= 0
          for: 1h
          labels:
            severity: warning

      - name: network
        rules:

        - alert: CustomNetworkInterfaceErrors
          annotations:
            summary: "{{ $labels.cluster }} Node network transmit errors"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22increase(node_network_transmit_errs_total%5B1h%5D)%20%3E%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $value }} node network transmit errors occured.
          expr: increase(node_network_transmit_errs_total[1h]) > 0
          for: 1m
          labels:
            severity: warning

      - name: ipmi
        rules:

        - alert: CustomIpmiServerPowerSupplyFailure
          annotations:
            summary: "{{ $labels.cluster }} Server power supply failure"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22ipmi_power_state%7B%7D%20%3E%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster }} on instance {{$labels.instance }} has a non-zero power supply state {{ $value }}.
          expr: ipmi_power_state{} > 0
          for: 1m
          labels:
            severity: warning

        - alert: CustomIpmiChassisPowerSupplyFailure
          annotations:
            summary: "{{ $labels.cluster }} Chassis power supply failure"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22ipmi_chassis_power_state%7B%7D%20%3D%3D%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster}} on instance {{$labels.instance }} chassis power is off.
          expr: ipmi_chassis_power_state{} == 0
          for: 1m
          labels:
            severity: warning

        - alert: CustomIpmiServerMemoryError
          annotations:
            summary: "{{ $labels.cluster }} Server memory error"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22ipmi_sensor_state%7Btype%3D%5C%22Memory%5C%22%7D%20%3E%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster}} on instance {{$labels.instance }} has a non-zero memory state {{ $value }}.
          expr: ipmi_sensor_state{type="Memory"} > 0
          for: 1m
          labels:
            severity: warning

        - alert: CustomIpmiLocalDiskError
          annotations:
            summary: "{{ $labels.cluster }} Local disk error"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22ipmi_sensor_state%7Btype%3D%5C%22Drive%20Slot%5C%22%7D%20%3E%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster}} on instance {{$labels.instance }} has a non-zero drive slot state {{ $value }}.
          expr: ipmi_sensor_state{type="Drive Slot"} > 0
          for: 1m
          labels:
            severity: warning

        - alert: CustomIpmiInternalCoolingFanError
          annotations:
            summary: "{{ $labels.cluster}} Internal cooling fan error on {{$labels.instance }}"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22ipmi_fan_speed_state%7B%7D%20%3E%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster}} on instance {{$labels.instance }} has a non-zero fan speed state {{ $value }}.
          expr: ipmi_fan_speed_state{} > 0
          for: 1m
          labels:
            severity: warning

      - name: node
        rules:

        - alert: CustomNodeNotReady
          annotations:
            summary: "{{ $labels.cluster}} node {{$labels.node }} not in ready status"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22kube_node_status_condition%7Bcondition%3D%5C%22Ready%5C%22,%20status!%3D%5C%22true%5C%22%7D%20%3E%200%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster}} node {{$labels.node }} is not in ready status.
          expr: kube_node_status_condition{condition="Ready", status!="true"} > 0
          for: 1m
          labels:
            severity: warning

        - alert: CustomNodeLowAvailableMemory
          annotations:
            summary: "{{ $labels.cluster}} node {{$labels.instance }} low on memory"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-1h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22node_memory_MemAvailable_bytes%2Fnode_memory_MemTotal_bytes%20%3C%200.2%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              {{ $labels.cluster}} node {{$labels.instance}} is low on memory.
          expr: node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes < 0.2
          for: 1m
          labels:
            severity: warning

      - name: opelimits
        rules:

        - alert: CustomOpeLimitsCpu
          annotations:
            summary: "{{ $labels.cluster }} - ope CPU usage above 80% of limit"
            description: |
              <https://grafana-open-cluster-management-observability.apps.nerc-ocp-infra.rc.fas.harvard.edu/explore?orgId=1&left=%5B%22now-3h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22sum(avg_over_time(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate%5B1h%5D))%20by%20(cluster,%20namespace,%20pod)%20%2F%20avg(kube_pod_container_resource_limits%7Bresource%3D%5C%22cpu%5C%22%7D)%20by%20(cluster,%20namespace,%20pod)%20%3E%200.80%22%7D%5D|Click here to see these metrics in Observability Monitoring>
              CPU usage is at {{ $value | humanizePercentage }} in namespace {{ $labels.namespace }}, cluster {{ $labels.cluster }}.
          expr: sum(avg_over_time(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.cpu",type="used"}[10m:5m])) / sum(avg_over_time(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.cpu",type="hard"}[10m:5m])) > 0.80
          for: 10m
          labels:
            severity: warning
            cpu: limit

      - name: ope6amStatus
        rules:

        - alert: Custom6amOpeLimitsCpu
          # A limits.cpu percentage of limit
          expr: (max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.cpu",type="used"})[10m:5m])/max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.cpu",type="hard"})[10m:5m])) and (hour()-5+(minute()/60)==6.0)
          # No 'for' clause when firing immediately, time is always utc -5 to calculate New York time (summer winter time switch will be wrong by an hour for a week)
          labels:
            severity: info
          annotations:
            summary: "{{ $labels.cluster }} - current status in namespace rhods-notebooks, cluster nerc-ocp-prod"
            description: |
              info: limits.cpu: {{ $value | humanizePercentage }} used

        - alert: Custom6amOpeLimitsEphemeralStorage
          # B limits.ephemeral-storage percentage of limit
          expr: (max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.ephemeral-storage",type="used"})[10m:5m])/max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.ephemeral-storage",type="hard"})[10m:5m])) and (hour()-5+(minute()/60)==6.0)
          labels:
            severity: info
          annotations:
            description: |
              info: limits.ephemeral-storage: {{ $value | humanizePercentage }} used

        - alert: Custom6amOpeLimitsMemory
          # C limits.memory percentage of limit
          expr: (max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.memory",type="used"})[10m:5m])/max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="limits.memory",type="hard"})[10m:5m])) and (hour()-5+(minute()/60)==6.0)
          labels:
            severity: info
          annotations:
            description: |
              info: limits.memory: {{ $value | humanizePercentage }} used

        - alert: Custom6amOpePersistentVolumeClaims
          # D persistentvolumeclaims percentage of limit
          expr: (max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="persistentvolumeclaims",type="used"})[10m:5m])/max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="persistentvolumeclaims",type="hard"})[10m:5m])) and (hour()-5+(minute()/60)==6.0)
          labels:
            severity: info
          annotations:
            description: |
              info: persistentvolumeclaims: {{ $value | humanizePercentage }} used

        - alert: Custom6amOpeRequestsStorage
          # E requests.storage percentage of limit
          expr: (max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="requests.storage",type="used"})[10m:5m])/max_over_time(sum(kube_resourcequota{cluster="nerc-ocp-prod",namespace="rhods-notebooks",resource="requests.storage",type="hard"})[10m:5m])) and (hour()-5+(minute()/60)==6.0)
          labels:
            severity: info
          annotations:
            description: |
              info: requests.storage: {{ $value | humanizePercentage }} used

        - alert: Custom6amOpeKubePodContainerInfo
          # F kube_pod_container_info absolute
          expr: (max_over_time(count(kube_pod_container_info{cluster="nerc-ocp-prod",namespace="rhods-notebooks"})[10m:5m])) and ((hour()-5+(minute()/60))==6.0)
          labels:
            severity: info
          annotations:
            description: |
              info: kube_pod_container_info: {{ $value }} containers

        - alert: Custom6amOpeKubePodOwner
          # G kube_pod_owner absolute
          expr: (max_over_time(count(kube_pod_owner{cluster="nerc-ocp-prod",namespace="rhods-notebooks"})[10m:5m])) and ((hour()-5+(minute()/60))==6.0)
          labels:
            severity: info
          annotations:
            description: |
              info: kube_pod_owner: {{ $value }} pod owners

# Prometheus generates a metric called up that indicates whether a scrape was successful.
# A value of “1” is scrape indicates success, “0” failure.
# The up metric is useful for debugging and alerting for targets that are down or having issues.
# Each target should produce an up metric on every scrape.
# I'm not sure which of the many metrics we want to check are up, but there are many here:
# https://multicloud-console.apps.nerc-ocp-infra.rc.fas.harvard.edu/grafana/explore?orgId=1&left=%5B%22now-1h%22,%22now%22,%22Observatorium%22,%7B%22exemplar%22:true,%22expr%22:%22up%7Bpod!%3D%5C%22%5C%22%7D%20%3D%3D%200%22%7D%5D
#        - alert: CustomInstanceDown
#          annotations:
#            summary: "Instance [{{ $labels.instance }}] down"
#            description: "[{{ $labels.instance }}] of job [{{ $labels.job }}] has been down for more than 15 minute."
#          expr: up == 0
#          for: 15m
#          labels:
#            severity: critical
